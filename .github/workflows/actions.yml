name: Deploy Infrastructure

on:
  workflow_dispatch:
    inputs:
      ENVIRONMENT: # variable
        description: 'Deploy Environment (dev/prod)'
        required: true
        default: 'dev'
      DEPLOY_CLUSTERS: # variable
        description: 'Create Clusters (true/false)'
        required: true
        default: 'true'
      DEPLOY_UC_STORAGE_CRED: # variable
        description: 'Create UC Storage Credential (true/false)'
        required: true
        default: 'true'

  push:
    branches:
      - main

jobs:
  deploy:
    name: 'Terraform Plan and Apply'
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./terraform

    env:
      TF_VAR_environment: ${{ github.event.inputs.ENVIRONMENT || secrets.ENVIRONMENT  }}
      TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
      TF_VAR_aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      TF_VAR_aws_access_key_secret: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      TF_VAR_databricks_account_id: ${{ secrets.DATABRICKS_ACCOUNT_ID }}
      TF_VAR_databricks_instances: '{"dev": "${{ secrets.DATABRICKS_INSTANCE_DEV }}", "prod": "${{ secrets.DATABRICKS_INSTANCE_PROD }}"}'
      TF_VAR_databricks_client_ids: '{"dev": "${{ secrets.DATABRICKS_CLIENT_ID_DEV }}", "prod": "${{ secrets.DATABRICKS_CLIENT_ID_PROD }}"}'
      TF_VAR_databricks_client_secrets: '{"dev": "${{ secrets.DATABRICKS_CLIENT_SECRET_DEV }}", "prod": "${{ secrets.DATABRICKS_CLIENT_ID_SECRET_PROD }}"}'
      TF_VAR_databricks_tokens: '{"dev": "${{ secrets.DATABRICKS_TOKEN_DEV }}", "prod": "${{ secrets.DATABRICKS_TOKEN_PROD }}"}'
      TF_VAR_databricks_admin_login: ${{ secrets.DATABRICKS_ADMIN_LOGIN }}
      TF_VAR_databricks_admin_password: ${{ secrets.DATABRICKS_ADMIN_PASSWORD }}
      # default to no if running directly from push to main branch so we execute a simple workflow run unit test
      # if running workflow manually then you can deploy clusters
      TF_VAR_databricks_deploy_clusters: ${{ github.event.inputs.DEPLOY_CLUSTERS == 'true' || 'false' }}
      # if running workflow manually then you can deploy uc storage credentials
      TF_VAR_databricks_deploy_uc_storage_credential: ${{ github.event.inputs.DEPLOY_UC_STORAGE_CRED == 'true' || 'false' }}
      TF_VAR_github_actor: ${{ github.actor }}

    steps:

      - name: Checkout repository
        uses: actions/checkout@v3
    
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1

      - name: Terraform Init Backend
        run: |
          export BUCKET_NAME="dbricks-dev-bucket"
          export KEY_NAME="terraform/terraform.tfstate"
          export AWS_REGION="${{ env.TF_VAR_aws_region }}"
          export ACCESS_KEY="${{ env.TF_VAR_aws_access_key_id }}"
          export SECRET_KEY="${{ env.TF_VAR_aws_access_key_secret}}"
          chmod +x ../terraform/backend.sh
          ../terraform/backend.sh

      - name: Terraform Init
        run: |
          export TF_LOG=DEBUG
          terraform init

      - name: Terraform Validate
        run: |
          export TF_LOG=DEBUG
          terraform validate
      
      - name: Terraform Plan
        run: |
          export TF_LOG=DEBUG
          terraform plan

      - name: Deploy Databricks Clusters
        if: env.TF_VAR_databricks_deploy_clusters == 'true'
        run: |
          terraform apply --auto-approve -target=module.cluster_module.databricks_cluster.this

      - name: Deploy Unity Catalog Storage Credential
        if: env.TF_VAR_databricks_deploy_uc_storage_credential == 'true'
        run: |
          terraform apply --auto-approve -target=module.uc_sc_module.databricks_storage_credential.external
          terraform apply --auto-approve -target=module.uc_sc_module.databricks_grants.external_creds